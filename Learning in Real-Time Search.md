## [2006] 实时搜索学习：一个统一的框架 ⭐⭐⭐⭐

**2022.03.30**

论文名称：Learning in Real-Time Search: A Unifying Framework，Journal of Artificial Intelligence Research 25 (2006) 119-157

作者来自阿尔伯塔大学

[论文链接](https://arxiv.org/pdf/1110.4076.pdf)


> **摘要：** 实时搜索方法适用于智能体实时与初始交互任务未知环境。在这样的同时规划和学习问题中，智能体必须在有限的时间内选择其动作，同时仅感知以智能体当前位置为中心的环境的局部部分。实时启发式搜索代理使用有限的前瞻搜索和使用启发式函数评估边界状态。在重复的经验中，他们改进了状态的启发式值以避免无限循环并收敛到更好的解决方案。这种设置在自主软件和硬件代理中的广泛传播已经在过去的二十年里，导致了实时搜索算法的爆炸式增长。不仅有潜力用户面临着算法的大杂烩，但也面临着控制参数的选择使用。在本文中，我们解决了这两个问题。第一个贡献是引入了一个简单的三参数框架（名为 LRTS），它提取了许多现有的核心思想算法。然后我们证明 LRTA*、ε-LRTA*、SLA* 和 γ-Trap 算法是特殊情况我们的框架。因此，它们被统一并扩展了附加功能。二、我们证明LRTS框架涵盖的任何算法的完整性和收敛性。第三，我们证明几个与控制参数和解决方案质量相关的上界。最后，我们分析三个控制参数在现实可扩展实时域中的经验影响在商业角色扮演游戏中最初未知的地图上导航以及在自组织传感器网络。

### 6 LRTS: A Unifying Framework

在本节中，我们将介绍本文的主要贡献——统一框架实时启发式搜索——以增量方式。 即，我们将从基础开始算法，LRTA*，然后分析三个扩展：更深的前瞻、最优权重和回溯。 每个扩展都用手工追踪的微型示例和经验结果进行说明在实时寻路域中。 一个统一的算法构成了该部分的结果。

#### 6.1 Learning Real-Time A* (LRTA*)

LRTA* 由 Korf (1990) 引入，是第一个也是最著名的学习实时启发式搜索算法。关键思想在于交错执行（interleaving acting）和备份启发式值（backing up heuristic values）。具体来说，在当前状态 s 中，先行为 1 的 LRTA* 考虑直接邻居（图 2 中的第 4-5 行）。 对于每个相邻状态，计算两个值：从当前状态到达那里的执行成本（以下用 g 表示）和从相邻状态到达最近目标状态的估计执行成本（以下用 h 表示）。 虽然 g 是已知的，准确地说，h 是一个启发式估计。 然后 LRTA* 移动到具有最低 f = g + h 值的状态（第 7 行）。 此外，如果最小 f 值大于当前状态的启发式值（第 6 行），它会更新当前状态的启发式值。

![image](https://user-images.githubusercontent.com/68068739/160790477-d837b856-9dbe-4abb-b19a-2458d6f41645.png)

Korf (1990) 表明，在有限域中，目标状态可以从任何状态到达，并且所有行动成本都是非零的，LRTA* 在每次试验中找到一个目标。 此外，如果初始启发式函数是可接受的，则 LRTA* 将在有限次数后收敛到最优解试验。 与 A* 搜索相比，前瞻为 1 的 LRTA* 具有相当短的第一步滞后，并且可以更快地找到次优解决方案。 然而，收敛到最优（例如，最低执行成本）解决方案在试验次数和总试验次数方面可能很昂贵执行成本。 

#### 6.2 Extension 1: Deeper Lookahead Search

LRTA* 遵循启发式景观到局部最小值。 它通过提高避免卡在那里启发式值并最终消除局部最小值。 因此，局部启发式最小值，由初始启发式值的不准确引起的，通过学习过程消除。Ishida (1997) 将启发式中的这种不准确性称为“启发式抑郁症”。 他学习它们用于 LRTA* 的基本情况，前瞻为 1。 启发式凹陷后来被概括为加权 LRTA* 和任意前瞻的情况（Bulitko，2004），名称为
γ-陷阱。

![image](https://user-images.githubusercontent.com/68068739/160792942-b147f12e-df71-4181-8399-60e2efd8b65e.png)

在“填充”启发式凹陷的过程中，代理会产生大量的执行成本通过在凹陷内来回移动。 发生这种情况是因为 LRTA* 具有一个人的前瞻是短视的，并且每次移动都会进行最少的计划。 如果计划是
比执行便宜，那么一个自然的解决方案是增加每次移动的计划量希望以较低的执行成本消除启发式局部最小值。

在 LRTA* 中，每次移动的额外规划是通过更深入的前瞻搜索来实现的。 上一节介绍的启发式更新和动作选择规则可以扩展为以游戏中标准最小最大搜索（Korf，1990）。Korf 将新规则称为“最小最小值”，并根据经验观察到，更深的前瞻性会降低执行成本，但会增加每次移动的计划成本。这种现象可以用图 4 中的一个可手动追踪的示例来说明。六种状态中的每一种都显示为一个圆圈，其中显示为边缘的动作。 每个动作的成本为 1。初始启发式 h0 是可接受的，但不准确。 第一次试验之前和之后的启发式值显示为每个状态下的数字
（圆圈）。








